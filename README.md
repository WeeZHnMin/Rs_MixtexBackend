
<p align="center">
  <img src="icon.ico" width="450" height="450">
</p>

## é¡¹ç›®ä»‹ç» | Project Introduction

æœ¬é¡¹ç›®åŸºäºŽ Rust è¯­è¨€å¼€å‘ï¼Œä½¿ç”¨ `ort` ä¸Ž `tokenizers` æ¨¡å—å®žçŽ° OCR æŽ¨ç†ï¼ŒåŽç«¯æ¡†æž¶é‡‡ç”¨ [Axum](https://github.com/tokio-rs/axum)ã€‚

æŽ¨ç†æ¨¡åž‹ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šEncoder ä¸º SwinModelï¼ŒDecoder ä¸º GPT-2 æ¨¡åž‹ã€‚æ¨¡åž‹æ‰˜ç®¡åœ¨ Hugging Face ä»“åº“ï¼š[https://huggingface.co/MixTex/base\_ZhEn](https://huggingface.co/MixTex/base_ZhEn)

> This project is developed in **Rust**, using the `ort` and `tokenizers` crates to implement OCR inference. The backend is built with **Axum** framework.
> The inference model consists of two components:
>
> * **Encoder**: SwinModel
> * **Decoder**: GPT-2
>   The model is hosted on Hugging Face: [https://huggingface.co/MixTex/base\_ZhEn](https://huggingface.co/MixTex/base_ZhEn)

---

## ä¸‹è½½æ¨¡åž‹ | Pulling the Model Files

è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸­æ‰“å¼€ PowerShellï¼Œç„¶åŽæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```powershell
mkdir backend\models
Invoke-WebRequest -Uri "https://huggingface.co/wzmmmm/_wmzmz/resolve/main/encoder_model.onnx" -OutFile "models/encoder_model.onnx"
Invoke-WebRequest -Uri "https://huggingface.co/wzmmmm/_wmzmz/resolve/main/decoder_model.onnx" -OutFile "models/decoder_model.onnx"
```

> Run the above commands in **PowerShell** to download the model files into the local `models/` directory.

å¦‚æžœæ— æ³•ä½¿ç”¨ PowerShell ä¸‹è½½ï¼Œè¯·æ‰‹åŠ¨è®¿é—®ä»¥ä¸‹é“¾æŽ¥ä¸‹è½½æ–‡ä»¶ï¼š

* ðŸ”— [https://huggingface.co/wzmmmm/\_wmzmz/tree/main](https://huggingface.co/wzmmmm/_wmzmz/tree/main)

ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶ï¼Œå¹¶æ”¾å…¥é¡¹ç›®çš„ `models/` ç›®å½•ä¸­ï¼ˆå¦‚è¯¥ç›®å½•ä¸å­˜åœ¨è¯·è‡ªè¡Œåˆ›å»ºï¼‰ï¼š

* `encoder_model.onnx`
* `decoder_model.onnx`

> If you cannot download via PowerShell, please download them manually from the link above and place them under the `models/` directory.

---

## å¯åŠ¨æœåŠ¡ | Running the Project

### æž„å»ºé¡¹ç›® | Build

```cmd
cargo build
```

### å¯åŠ¨é¡¹ç›® | Run

```cmd
cargo run
```

å½“ä½ çœ‹åˆ°ç¨‹åºç›‘å¬åœ¨ `localhost:8000`ï¼Œè¯´æ˜Žæ¨¡åž‹å·²ç»åŠ è½½å®Œæˆå¹¶å¼€å§‹æä¾›æŽ¨ç†æœåŠ¡ã€‚

> After running the program, if you see the model listening at `localhost:8000`, it means the service is running successfully and the model is ready.
