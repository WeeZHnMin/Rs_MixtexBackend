ä»¥ä¸‹æ˜¯ä½ æä¾›çš„ Markdown æ–‡æ¡£å†…å®¹çš„æ¶¦è‰²ä¸ä¸­è‹±ä¼˜åŒ–ç‰ˆæœ¬ï¼Œè¯­æ³•æ›´å‡†ç¡®ã€è¡¨è¾¾æ›´è‡ªç„¶ï¼Œé€‚åˆç”¨äºæ­£å¼çš„å¼€æºé¡¹ç›®è¯´æ˜æ–‡æ¡£ï¼ˆREADME.mdï¼‰ï¼š

---

<p align="center">
  <img src="assets/icon.ico" width="450" height="450">
</p>

## é¡¹ç›®ä»‹ç» | Project Introduction

æœ¬é¡¹ç›®åŸºäº Rust è¯­è¨€å¼€å‘ï¼Œä½¿ç”¨ `ort` ä¸ `tokenizers` æ¨¡å—å®ç° OCR æ¨ç†ï¼Œåç«¯æ¡†æ¶é‡‡ç”¨ [Axum](https://github.com/tokio-rs/axum)ã€‚

æ¨ç†æ¨¡å‹ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šEncoder ä¸º SwinModelï¼ŒDecoder ä¸º GPT-2 æ¨¡å‹ã€‚æ¨¡å‹æ‰˜ç®¡åœ¨ Hugging Face ä»“åº“ï¼š[https://huggingface.co/MixTex/base\_ZhEn](https://huggingface.co/MixTex/base_ZhEn)

> This project is developed in **Rust**, using the `ort` and `tokenizers` crates to implement OCR inference. The backend is built with **Axum** framework.
> The inference model consists of two components:
>
> * **Encoder**: SwinModel
> * **Decoder**: GPT-2
>   The model is hosted on Hugging Face: [https://huggingface.co/MixTex/base\_ZhEn](https://huggingface.co/MixTex/base_ZhEn)

---

## ä¸‹è½½æ¨¡å‹ | Pulling the Model Files

è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸­æ‰“å¼€ PowerShellï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```powershell
mkdir backend\models
Invoke-WebRequest -Uri "https://huggingface.co/wzmmmm/_wmzmz/resolve/main/encoder_model.onnx" -OutFile "models/encoder_model.onnx"
Invoke-WebRequest -Uri "https://huggingface.co/wzmmmm/_wmzmz/resolve/main/decoder_model.onnx" -OutFile "models/decoder_model.onnx"
```

> Run the above commands in **PowerShell** to download the model files into the local `models/` directory.

å¦‚æœæ— æ³•ä½¿ç”¨ PowerShell ä¸‹è½½ï¼Œè¯·æ‰‹åŠ¨è®¿é—®ä»¥ä¸‹é“¾æ¥ä¸‹è½½æ–‡ä»¶ï¼š

* ğŸ”— [https://huggingface.co/wzmmmm/\_wmzmz/tree/main](https://huggingface.co/wzmmmm/_wmzmz/tree/main)

ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶ï¼Œå¹¶æ”¾å…¥é¡¹ç›®çš„ `models/` ç›®å½•ä¸­ï¼ˆå¦‚è¯¥ç›®å½•ä¸å­˜åœ¨è¯·è‡ªè¡Œåˆ›å»ºï¼‰ï¼š

* `encoder_model.onnx`
* `decoder_model.onnx`

> If you cannot download via PowerShell, please download them manually from the link above and place them under the `models/` directory.

---

## å¯åŠ¨æœåŠ¡ | Running the Project

### æ„å»ºé¡¹ç›® | Build

```cmd
cargo build
```

### å¯åŠ¨é¡¹ç›® | Run

```cmd
cargo run
```

å½“ä½ çœ‹åˆ°ç¨‹åºç›‘å¬åœ¨ `localhost:8000`ï¼Œè¯´æ˜æ¨¡å‹å·²ç»åŠ è½½å®Œæˆå¹¶å¼€å§‹æä¾›æ¨ç†æœåŠ¡ã€‚

> After running the program, if you see the model listening at `localhost:8000`, it means the service is running successfully and the model is ready.
